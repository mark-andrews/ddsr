---
title: "Chapter 13: Nonlinear Regression"
author: |
  | Mark Andrews
  | Psychology Department, Nottingham Trent University
  | 
  | \faEnvelopeO\  ```mark.andrews@ntu.ac.uk```
fontsize: 10pt
output:
 beamer_presentation:
  keep_tex: true
  fonttheme: "serif"
bibliography:
  - "`r system('kpsewhich mjandrews.bib', intern=TRUE)`"
biblio-style: apalike   
header-includes:
  - \input{header.tex}
---


```{r, echo=F}
knitr::opts_chunk$set(echo = F, prompt = F, warning = F, message = F, comment='#>')
# Thanks to 
# https://github.com/ramnathv/slidify/issues/189#issuecomment-15850008
hook1 <- function(x){ gsub("```\n*```r*\n*", "", x) }
hook2 <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = hook1)
```

```{r}
library(tidyverse)
library(magrittr)
library(knitr)
library(modelr)
library(cowplot)
library(tikzDevice)
theme_set(theme_classic())
```



# Introduction

The normal linear model is as follows:
$$
y_i \sim N(\mu_i, \sigma^2),\quad \mu_i = \beta_0 + \sum_{k=1}^K \beta_k x_{ik},\quad\text{for $i \in 1\ldots n$}.
$$


We can define a normal *nonlinear* regression model as follows:
$$
y_i \sim N(\mu_i, \sigma^2),\quad \mu_i = f(x_i, \theta),\quad\text{for $i \in 1\ldots n$},
$$
where $f$ is some (deterministic) nonlinear function of $x_i$, and $\theta$ is a set of parameters of $f$.

More generally, of course, we do not have to assume our outcome variable is normally distributed.
This leads to the following more general nonlinear regression model:
$$
y_i \sim D(\mu_i, \psi),\quad \mu_i = f(x_i, \theta),\quad\text{for $i \in 1\ldots n$},
$$
where $D$ is some probability distribution with parameters $\mu_i$ and $\psi$, and $\mu_i$ is the nonlinear function of $x_i$, which is parameterized by $\theta$.


# Polynomial regression

A normal linear regression model can be defined as follows:
$$
y_i \sim N(\mu_i, \sigma^2),\quad \mu_i = \beta_0 + \beta_1 x_i,\quad\text{for $i \in 1\ldots n$}.
$$

The function $\mu_i = \beta_0 + \beta_1 x_i = \beta_0 x_i^0 + \beta_1 x^1_i$ is a *polynomial of degree one*. 

By contrast, a degree $K = 5$ polynomial of each $x_i$.
\begin{align*}
y_i \sim N(\mu_i, \sigma^2),\quad 
\mu_i &= \beta_0 x_i^0 + \beta_1 x_i^1 + \beta_2 x_i^2 + \beta_3 x_i^3 + \beta_4 x_i^4 + \beta_5 x_i^5 ,\\
\mu_i &= \sum_{k=0}^K \beta_k x_i^k,\quad\text{for $i \in 1\ldots n$}.
\end{align*}

A polynomial regression model of single predictor is simply a linear regression model with multiple predictors, each of which being the predictor raised to a different power.

# Polynomial regression in practice

```{r, eyefix_fig_1, out.width='0.65\\textwidth',fig.align='center', fig.cap='Average proportion of eye fixations at a target in each time window in multisecond experimental trial.'}
eyefix_df <- read_csv('data/funct_theme_pts.csv')
eyefix_df_avg <- eyefix_df %>% 
  group_by(Time, Object) %>% 
  summarize(mean_fix = mean(meanFix)) %>% 
  ungroup()

eyefix_df_avg_targ <- filter(eyefix_df_avg, Object == 'Target')

eyefix_df_avg_targ %>% 
  ggplot(mapping = aes(x = Time, y = mean_fix)) +
  geom_point() 
```

#

To perform a polynomial regression in R, we can use `lm`. 
We can create the formula more easily using `poly` as follows.
```
y ~ poly(x, degree=3, raw=T)
```

In the following code, we perform polynomial regression on this data from degree $1$ (which is a standard linear model) to degree $9$. 

```{r, echo=T}
M_eyefix_targ <- map(seq(9), 
  ~lm(mean_fix ~ poly(Time, degree = ., raw = T), 
      data = eyefix_df_avg_targ)
)
```
The fitted models can be seen in Figure \ref{fig:eyefix_mod_1_fits} and the model fit statistics are show in Table \ref{tab:eyefix_mod_1_fits}.

#

```{r, results='asis'}
imap(M_eyefix_targ,
    ~c(.y, summary(.x)$r.sq, summary(.x)$adj.r.sq, logLik(.x), AIC(.x))
) %>% 
  do.call(rbind, .) %>% 
  round(2) %>% 
  set_colnames(c('degree', 'Rsq', 'Adj Rsq', 'LL', 'AIC')) %>% 
  as_tibble() %>% 
  kable(format = "latex", booktabs = T, caption = "Model fit statistics for polynomial regression models.", table.envir = "table",linesep = "", label='eyefix_mod_1_fits')
```

# 
```{r eyefix_mod_1_fits, out.width='\\textwidth',fig.align='center', fig.cap='The predicted functions in a sequences of polynomial models, from degree 1 to degree 9.'}
map(M_eyefix_targ, predict) %>% 
  as_tibble(.name_repair = 'minimal') %>% 
  set_colnames(paste0('degree_', seq(length(M_eyefix_targ)))) %>% 
  cbind(eyefix_df_avg_targ, .) %>% 
  gather(degree, prediction, starts_with('degree_')) %>% 
  ggplot(mapping = aes(x = Time)) +
  geom_point(aes(y = mean_fix), size = 0.5, alpha = 0.5) +
  geom_line(aes(y = prediction), col='black') +
  facet_wrap(~degree) +
  theme_minimal()
```


# Orthogonal polynomials

In the polynomials we have just run, we used `poly` with `raw = T`. 
By setting `raw = F`, which is the default, we obtain *orthogonal polynomials*.
This means that the predictor variables that represent the various powers of the predictor `Time` predictor are uncorrelated with one another.

Using orthogonal polynomials has computational and conceptual consequences.

Computationally, it avoids any multicollinearity.

Conceptually, on the other hand, orthogonal predictor variables entail that the coefficient for each predictor is the same regardless of which of the other predictors are present, or indeed whether any of the other predictor are present.

# Spline and basis function regression

In basis function regression, we model the nonlinear functions of the predictors variables using linear combinations of simpler functions that are known as the basis functions.

For example, in the case of a nonlinear regression with one predictor variable, and assuming normally distributed outcome variables, we would write our basis function regression model as follows:
\begin{align*}
y_i \sim N(\mu_i, \sigma^2),\quad 
&\mu_i = f(x_i) = \beta_0 + \sum_{k=1}^K \beta_k \phi_k(x_i),\quad\text{for $i \in 1\ldots n$}.
\end{align*}
Here, $\phi_1(x_i), \phi_2(x_i) \ldots \phi_k(x_i) \ldots \phi_K(x_i)$ are (usually) simple deterministic functions of $x_i$.

```{r}
b_spline <- function(x, knots, show_piece = F){
  
  stopifnot(length(knots) == 5)
  
  .b_spline <- function(x){
    if (x >= knots[1] & x < knots[2]) {
      piece <- 1
      u <- (x-knots[1])/(knots[2] - knots[1])
      y <- 1/6 * u^3 
      
    } else if (x >= knots[2] & x < knots[3]) {
      piece <- 2
      u <- (x-knots[2])/(knots[3] - knots[2])
      y <- 1/6 * (1 + 3*u + 3*u^2 - 3*u^3)
      
    } else if (x >= knots[3] & x < knots[4]) {
      piece <- 3
      u <- (x-knots[3])/(knots[4] - knots[3])
      y <- 1/6 * (4 - 6*u^2 + 3*u^3)
      
    } else if (x >= knots[4] & x <= knots[5]) {
      piece <- 4
      u <- (x-knots[4])/(knots[5] - knots[4])
      y <- 1/6 * (1 - 3*u + 3*u^2 - u^3)
    }
    else {
      piece <- 0
      y <- 0 
    } 
    
    if (!show_piece) return(y)
    
    c(y, piece)
  
  }
  
  if (!show_piece){
    tibble(x = x, 
           y = map_dbl(x, .b_spline)
    )
  } else {
    map(x, .b_spline) %>% 
      do.call(rbind, .)%>%
      set_colnames(c('y', 'segment')) %>% 
      as_tibble() %>% 
      mutate(x = x) %>% 
      mutate_at(vars(segment), as.factor) %>% 
      select(x, everything())
  }
  
}
```

# Cublic b-splines 

* There are many different types of basis functions that are possible to use, but one particularly widely used class of basis functions are *spline* functions.
* Splines are smooth functions composed of multiple pieces, each of which is a polynomial.
* Even in the context of basis function regression, there are many types of spline functions that can be used, but one of the most commonly used types is *cubic b-splines*.
* The *b* refers to *basis* and the *cubic* is the order of the polynomials that make up the pieces.
* Each cubic b-spline basis function is defined by 4 curve segments that join together smoothly.
* The breakpoints between the intervals on which these curves are defined are known as *knots*.

# Cublic b-splines 

The cubic b-spline is then defined as follows:
\scriptsize
$$
\phi_k(x_i) = 
\begin{cases}
\tfrac{1}{6} u^3, &\quad\text{if $x_i \in (t^k_0, t^k_1]$},\quad\text{with $u = (x_i-t^k_0)/(t^k_1-t^k_0)$}\\
\tfrac{1}{6} (1 + 3u + 3u^2 - 3u^3), &\quad\text{if $x_i \in (t^k_1, t^k_2]$},\quad\text{with $u = (x_i-t^k_1)/(t^k_2-t^k_1)$}\\ 
\tfrac{1}{6} (4 - 6u^2 + 3u^3), &\quad\text{if $x_i \in (t^k_2, t^k_3]$},\quad\text{with $u = (x_i-t^k_2)/(t^k_3-t^k_2)$}\\
\tfrac{1}{6} (1 - 3u + 3u^2 - u^3), &\quad\text{if $x_i \in (t^k_3, t^k_4)$},\quad\text{with $u = (x_i-t^k_3)/(t^k_4-t^k_3)$}\\
0 &\quad\text{if $x_i < t^k_0$ or $x_i > t^k_4$}
\end{cases}
$$
\normalsize

#

In Figure \ref{fig:bspline_basis}, we plot a single cubic b-spline basis function defined on the knots $\{-\tfrac{1}{2}, -\tfrac{1}{4}, 0, \tfrac{1}{4}, \tfrac{1}{2}\}$. 
In this figure, we have colour coded the curve segments.


```{r bspline_basis, fig.align = 'center', out.width='0.5\\textwidth', fig.cap="A single cubic b-spline basis function defined on the knots $\\{-\\tfrac{1}{2}, -\\tfrac{1}{4}, 0, \\tfrac{1}{4}, \\tfrac{1}{2}\\}$."}
x <- seq(-0.5, 0.5, length.out = 1000)
knots <- seq(-0.5, 0.5, length.out = 5)
b_spline(x, knots = knots, show_piece = T) %>%
  ggplot(mapping = aes(x = x, 
                       y = y, 
                       colour = segment)) +
  geom_line() +
  theme(legend.position="none")
```

# 

* The simplest way to perform spline regression in R is to use `splines::bs`, or a related function from the `splines` package, just as we used `poly` for polynomial regression.
* For example, to perform a cubic b-spline regression on our eye-fixation rates to the three object categories, we could do the following.
```{r, echo=T}
library(splines)
knots <- seq(-500, 2500, by = 500)
M_bs <- lm(mean_fix ~ bs(Time, knots = knots)*Object, 
           data=eyefix_df_avg)
```
The model fit for this model is shown in Figure \ref{fig:bspline_fit_1}.

# 

```{r, bspline_fit_1, out.width='0.6\\textwidth',fig.align='center', fig.cap='The fit of a cubic b-spline, with evenly spaced basis functions every 500ms, to the average eye fixation rates to each \\texttt{Object} category.'}
eyefix_df_avg%>%
  add_predictions(M_bs) %>%
  ggplot(mapping = aes(x = Time, group = Object, linetype = Object)) +
  geom_point(aes(y = mean_fix), size = 0.5, alpha = 0.5) +
  geom_line(aes(y = pred))
```

# Generalized additive models

* The polynomial and spline regression models that we have covered in the previous two sections can be regarded as special cases of a more general type of regression model known as a *generalized additive model* (\gam).

* Given $n$ observations of a set of $L$ predictor variables $x_1, x_2 \ldots x_l \ldots x_L$ and outcome variable $y$, a \gam regression model of this data is:
$$
y_i \sim D(\mu_i, \psi),\quad \mu_i = f_1(x_{1i}) + f_2(x_{2i}) + \ldots + f_L(x_{Li}),\quad\text{for $i \in 1\ldots n$},
$$
where $D$ is some probability distribution with parameters $\psi$, and each predictor variable $f_l$ is a *smooth function* of the predictor variable's values.
Usually each smooth function $f_l$ is a weighted sum of basis functions such as spline basis functions or other common types, some of which we describe below.
In other words, the smooth function $f_l$ might be defined as follows:
$$
f_l(x_{li}) = \beta_{l0} + \sum_{k=1}^K \beta_{lk} \phi_{lk}(x_{li}),
$$
where $\phi_{lk}$ is a basis function of $x_{li}$. 

# Using `mgcv`

* The R package `mgcv` [@mgcv:cran;@wood:gam:book] is a powerful and versatile toolbox for using \gams in R.
* We will use a classic data-set often used to illustrate nonlinear regression, namely the `mycle` data set, available in the `MASS` package and elsewhere.
```{r}
mcycle <- MASS::mcycle
```
```{r mcycle, out.width='0.5\\textwidth', fig.cap='Head acceleration over time in a simulated motorcycle crash.', fig.align='center'}
mcycle %>% 
  ggplot(aes(x = times, y = accel)) +
  geom_point()
```

#

* The main function we will use from `mgcv` is `gam`.
* In other to use `gam` to do basis function regression, we must apply what `mgcv` calls *smooth terms*. 
* Here, we will use the function simply named `s` to set up the basis functions. 
```{r, echo=TRUE}
library(mgcv)
M_1 <- gam(accel ~ s(times), data = mcycle)
```
The plot of this model in shown in Figure \ref{fig:mcycle_gam_1}.
```{r mcycle_gam_1, out.width='0.5\\textwidth', fig.cap='A thin plate spline basis function regression model applied to the \\texttt{mycle} data set.', fig.align='center'}
plot(M_1, residuals = T)
```

# References

